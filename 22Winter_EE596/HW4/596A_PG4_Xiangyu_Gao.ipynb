{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJzHJrMXDga9"
      },
      "source": [
        "In this short programming assignment we will look into applications of word embeddings in similarity search and nearest neighbors. We will also look at creating a video based on tSNE embeddings of images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGK0yf-5DgbB"
      },
      "source": [
        "## -1. Create a video from images\n",
        "Download any 1000 or more 'appropriate' and publicly available images from the web. This could be part of a data set or something specific that you picked up or are interested in.\n",
        "\n",
        "We discussed using tSNE to find image embeddings for these images. Apply the tSNE library to these images and construct low-dimensional embeddings for the images. Use these embeddings to then:\n",
        "\n",
        "a) Start at any random image in the data set \n",
        "\n",
        "b) Sequentially chain the next image to the previous image using a scoring function/probability based on the tSNE embedding. So you want to chain the most similar image to the current one and so on. Choose a frame rate that is appropriate to convert this chain of images into a video. Your video shouldn't be more than 3 minutes long.\n",
        "\n",
        "c) Upload this video to youtube and share a link with your submission. \n",
        "\n",
        "d) Feel free to share your video to Discord to see what cool videos we come up with!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=3nnE0nqffPc"
      ],
      "metadata": {
        "id": "GCIqt8iJW0f0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQNOqhdhDgbC"
      },
      "source": [
        "## Diving into Cheat Sheet of Pandas Data Frame\n",
        "There are some useful functions for solving problems below when it comes to index and slice data frames. Let's go over them.\n",
        "More materials can be found here: https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n",
        "1. DataFrame() Construct a dataframe. Use it for putting a numpy ndarray into a dataframe.\n",
        "1. DataFrame.loc() Purely label-location based indexer for selection by label. Use it for selecting word vectors in the dataframe.     \n",
        "1. DataFrame.dot() Matrix multiplication with DataFrame. Use it for dot product of word vectors.\n",
        "1. DataFrame.sort_values() Sort by the values along either axis. Use it for sorting distance short to long.\n",
        "\n",
        "Below are examples of using these functions. You don't have to code anything in this block, just focus on understanding the functions and how it works in pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKlLQZrJIOfP",
        "outputId": "cda2c342-6cb8-4e92-cfea-75e5990fdd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoY9Mf74DgbD",
        "outputId": "a2c7a15d-0759-40a7-b91b-851d08442692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define sample word vectors\n",
            "[[0.1 0.3 0.4 0.5]\n",
            " [0.3 0.4 0.9 0.5]\n",
            " [0.2 0.8 0.7 0.5]]\n",
            "\n",
            "Pandas Data Frame for word vectors\n",
            "         0    1    2    3\n",
            "word1  0.1  0.3  0.4  0.5\n",
            "word2  0.3  0.4  0.9  0.5\n",
            "word3  0.2  0.8  0.7  0.5\n",
            "\n",
            "Find the row corresponding to word1\n",
            "0    0.1\n",
            "1    0.3\n",
            "2    0.4\n",
            "3    0.5\n",
            "Name: word1, dtype: float64\n",
            "\n",
            "Calculate the dot product between word1 and word2\n",
            "0.76\n",
            "\n",
            "Calculate the dot product between word1 and rest of the words\n",
            "word2    0.76\n",
            "word3    0.79\n",
            "dtype: float64\n",
            "\n",
            "Sorted dot product values\n",
            "word3    0.79\n",
            "word2    0.76\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define a ndarray\n",
        "d = np.array([[0.1,0.3,0.4,0.5],[0.3,0.4,0.9,0.5],[0.2,0.8,0.7,0.5]], dtype=float, order='F')\n",
        "print(\"Define sample word vectors\")\n",
        "print(d)\n",
        "#Construct a dataframe from ndarray and index each row as word vectors\n",
        "df = pd.DataFrame(d,index = ['word1','word2','word3'])\n",
        "print(\"\\nPandas Data Frame for word vectors\")\n",
        "print(df)\n",
        "#Select word vector1 by its label\n",
        "print(\"\\nFind the row corresponding to word1\")\n",
        "print(df.loc['word1'])\n",
        "#Calculate dot product of word vector1 and word vector2\n",
        "print(\"\\nCalculate the dot product between word1 and word2\")\n",
        "dot_product = df.loc['word2'].dot(df.loc['word1'])\n",
        "print(dot_product)\n",
        "#Calculate dot product of word vector1 to the rest of words\n",
        "print(\"\\nCalculate the dot product between word1 and rest of the words\")\n",
        "words_rest = ['word2','word3']\n",
        "dot_product2 = df.loc[words_rest].dot(df.loc['word1'])\n",
        "print(dot_product2)\n",
        "#Sort Values of dot_product2 by high to low\n",
        "print(\"\\nSorted dot product values\")\n",
        "print(dot_product2.sort_values(ascending = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-wxvTxyDgbE"
      },
      "source": [
        "## Dataset Details - Standford's GloVe pre-trained word vectors\n",
        "\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus. The GloVe pre-trained word vectors dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. In this problem we are going to use the 50-dimensional dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9OACutkDgbF"
      },
      "source": [
        "## \\# 0. Get an overview on what Glove is\n",
        "Read up the documentation on glove embeddings, esp. where it gets applied here: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k30S1smDgbF"
      },
      "source": [
        "## Load Dataset\n",
        "Let's load the dataset first. Each row is indexed as a word vector. Dimension of word vectors is 50. How many words are there in this dataset? Print a few words and see what they are. You don't need to code anything here, just understand the data structure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "# Load GloVe pre-trained vectors \n",
        "local_file1='/content/drive/MyDrive/22WINTER/596A/HW4/glove.6B.50d.txt' # Make sure this file exists!\n",
        "df = pd.read_csv(local_file1,sep = ' ',index_col=0,header=None,engine='python',error_bad_lines=False,quoting = csv.QUOTE_NONE)\n",
        "print(\"dataset shape - Rows: %d, Cols: %d\" % (df.shape[0], df.shape[1]))\n",
        "words = list(df.index)\n",
        "print(\"print a few words in the dataset:\", words[30:40])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHLJxvAdw5ZA",
        "outputId": "bf521750-7e4f-41cc-bc2b-cfb3429969b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset shape - Rows: 400001, Cols: 50\n",
            "print a few words in the dataset: ['be', 'has', 'are', 'have', 'but', 'were', 'not', 'this', 'who', 'they']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1c_gv2uDgbG"
      },
      "source": [
        "## \\# 1. Print the first few 11 rows of the pandas data frame below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "zT3ceJpDDgbG",
        "outputId": "711f7434-599c-4003-cfb0-342867ec5c3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-77b92f34-8eef-4a64-bc0a-a93819cba6b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.249680</td>\n",
              "      <td>-0.41242</td>\n",
              "      <td>0.121700</td>\n",
              "      <td>0.345270</td>\n",
              "      <td>-0.044457</td>\n",
              "      <td>-0.49688</td>\n",
              "      <td>-0.178620</td>\n",
              "      <td>-0.000660</td>\n",
              "      <td>-0.656600</td>\n",
              "      <td>0.278430</td>\n",
              "      <td>-0.147670</td>\n",
              "      <td>-0.55677</td>\n",
              "      <td>0.146580</td>\n",
              "      <td>-0.00951</td>\n",
              "      <td>0.011658</td>\n",
              "      <td>0.102040</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>-0.84430</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>-0.016801</td>\n",
              "      <td>-0.332790</td>\n",
              "      <td>-0.155200</td>\n",
              "      <td>-0.231310</td>\n",
              "      <td>-0.191810</td>\n",
              "      <td>-1.8823</td>\n",
              "      <td>-0.767460</td>\n",
              "      <td>0.099051</td>\n",
              "      <td>-0.421250</td>\n",
              "      <td>-0.195260</td>\n",
              "      <td>4.0071</td>\n",
              "      <td>-0.185940</td>\n",
              "      <td>-0.522870</td>\n",
              "      <td>-0.316810</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.177780</td>\n",
              "      <td>-0.158970</td>\n",
              "      <td>0.012041</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>-0.298710</td>\n",
              "      <td>-0.157490</td>\n",
              "      <td>-0.347580</td>\n",
              "      <td>-0.045637</td>\n",
              "      <td>-0.442510</td>\n",
              "      <td>0.187850</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>-0.184110</td>\n",
              "      <td>-0.115140</td>\n",
              "      <td>-0.785810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.236820</td>\n",
              "      <td>-0.16899</td>\n",
              "      <td>0.409510</td>\n",
              "      <td>0.638120</td>\n",
              "      <td>0.477090</td>\n",
              "      <td>-0.42852</td>\n",
              "      <td>-0.556410</td>\n",
              "      <td>-0.364000</td>\n",
              "      <td>-0.239380</td>\n",
              "      <td>0.130010</td>\n",
              "      <td>-0.063734</td>\n",
              "      <td>-0.39575</td>\n",
              "      <td>-0.481620</td>\n",
              "      <td>0.23291</td>\n",
              "      <td>0.090201</td>\n",
              "      <td>-0.133240</td>\n",
              "      <td>0.078639</td>\n",
              "      <td>-0.41634</td>\n",
              "      <td>-0.154280</td>\n",
              "      <td>0.100680</td>\n",
              "      <td>0.488910</td>\n",
              "      <td>0.312260</td>\n",
              "      <td>-0.125200</td>\n",
              "      <td>-0.037512</td>\n",
              "      <td>-1.5179</td>\n",
              "      <td>0.126120</td>\n",
              "      <td>-0.024420</td>\n",
              "      <td>-0.042961</td>\n",
              "      <td>-0.283510</td>\n",
              "      <td>3.5416</td>\n",
              "      <td>-0.119560</td>\n",
              "      <td>-0.014533</td>\n",
              "      <td>-0.149900</td>\n",
              "      <td>0.218640</td>\n",
              "      <td>-0.334120</td>\n",
              "      <td>-0.138720</td>\n",
              "      <td>0.318060</td>\n",
              "      <td>0.703580</td>\n",
              "      <td>0.448580</td>\n",
              "      <td>-0.080262</td>\n",
              "      <td>0.630030</td>\n",
              "      <td>0.321110</td>\n",
              "      <td>-0.467650</td>\n",
              "      <td>0.227860</td>\n",
              "      <td>0.360340</td>\n",
              "      <td>-0.378180</td>\n",
              "      <td>-0.566570</td>\n",
              "      <td>0.044691</td>\n",
              "      <td>0.303920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.151640</td>\n",
              "      <td>0.301770</td>\n",
              "      <td>-0.16763</td>\n",
              "      <td>0.176840</td>\n",
              "      <td>0.317190</td>\n",
              "      <td>0.339730</td>\n",
              "      <td>-0.43478</td>\n",
              "      <td>-0.310860</td>\n",
              "      <td>-0.449990</td>\n",
              "      <td>-0.294860</td>\n",
              "      <td>0.166080</td>\n",
              "      <td>0.119630</td>\n",
              "      <td>-0.41328</td>\n",
              "      <td>-0.423530</td>\n",
              "      <td>0.59868</td>\n",
              "      <td>0.288250</td>\n",
              "      <td>-0.115470</td>\n",
              "      <td>-0.041848</td>\n",
              "      <td>-0.67989</td>\n",
              "      <td>-0.250630</td>\n",
              "      <td>0.184720</td>\n",
              "      <td>0.086876</td>\n",
              "      <td>0.465820</td>\n",
              "      <td>0.015035</td>\n",
              "      <td>0.043474</td>\n",
              "      <td>-1.4671</td>\n",
              "      <td>-0.303840</td>\n",
              "      <td>-0.023441</td>\n",
              "      <td>0.305890</td>\n",
              "      <td>-0.217850</td>\n",
              "      <td>3.7460</td>\n",
              "      <td>0.004228</td>\n",
              "      <td>-0.184360</td>\n",
              "      <td>-0.462090</td>\n",
              "      <td>0.098329</td>\n",
              "      <td>-0.119070</td>\n",
              "      <td>0.239190</td>\n",
              "      <td>0.116100</td>\n",
              "      <td>0.417050</td>\n",
              "      <td>0.056763</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.068987</td>\n",
              "      <td>0.087939</td>\n",
              "      <td>-0.102850</td>\n",
              "      <td>-0.139310</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>-0.080803</td>\n",
              "      <td>-0.356520</td>\n",
              "      <td>0.016413</td>\n",
              "      <td>0.102160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.708530</td>\n",
              "      <td>0.570880</td>\n",
              "      <td>-0.47160</td>\n",
              "      <td>0.180480</td>\n",
              "      <td>0.544490</td>\n",
              "      <td>0.726030</td>\n",
              "      <td>0.18157</td>\n",
              "      <td>-0.523930</td>\n",
              "      <td>0.103810</td>\n",
              "      <td>-0.175660</td>\n",
              "      <td>0.078852</td>\n",
              "      <td>-0.362160</td>\n",
              "      <td>-0.11829</td>\n",
              "      <td>-0.833360</td>\n",
              "      <td>0.11917</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>0.061555</td>\n",
              "      <td>-0.012719</td>\n",
              "      <td>-0.56623</td>\n",
              "      <td>0.013616</td>\n",
              "      <td>0.228510</td>\n",
              "      <td>-0.143960</td>\n",
              "      <td>-0.067549</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>-0.236980</td>\n",
              "      <td>-1.7037</td>\n",
              "      <td>-0.866920</td>\n",
              "      <td>-0.267040</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>0.176700</td>\n",
              "      <td>3.8676</td>\n",
              "      <td>-0.161300</td>\n",
              "      <td>-0.132730</td>\n",
              "      <td>-0.688810</td>\n",
              "      <td>0.184440</td>\n",
              "      <td>0.005246</td>\n",
              "      <td>-0.338740</td>\n",
              "      <td>-0.078956</td>\n",
              "      <td>0.241850</td>\n",
              "      <td>0.365760</td>\n",
              "      <td>-0.347270</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.075693</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.389880</td>\n",
              "      <td>0.229020</td>\n",
              "      <td>-0.216170</td>\n",
              "      <td>-0.225620</td>\n",
              "      <td>-0.093918</td>\n",
              "      <td>-0.803750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.680470</td>\n",
              "      <td>-0.039263</td>\n",
              "      <td>0.30186</td>\n",
              "      <td>-0.177920</td>\n",
              "      <td>0.429620</td>\n",
              "      <td>0.032246</td>\n",
              "      <td>-0.41376</td>\n",
              "      <td>0.132280</td>\n",
              "      <td>-0.298470</td>\n",
              "      <td>-0.085253</td>\n",
              "      <td>0.171180</td>\n",
              "      <td>0.224190</td>\n",
              "      <td>-0.10046</td>\n",
              "      <td>-0.436530</td>\n",
              "      <td>0.33418</td>\n",
              "      <td>0.678460</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>-0.344480</td>\n",
              "      <td>-0.42785</td>\n",
              "      <td>-0.432750</td>\n",
              "      <td>0.559630</td>\n",
              "      <td>0.100320</td>\n",
              "      <td>0.186770</td>\n",
              "      <td>-0.268540</td>\n",
              "      <td>0.037334</td>\n",
              "      <td>-2.0932</td>\n",
              "      <td>0.221710</td>\n",
              "      <td>-0.398680</td>\n",
              "      <td>0.209120</td>\n",
              "      <td>-0.557250</td>\n",
              "      <td>3.8826</td>\n",
              "      <td>0.474660</td>\n",
              "      <td>-0.956580</td>\n",
              "      <td>-0.377880</td>\n",
              "      <td>0.208690</td>\n",
              "      <td>-0.327520</td>\n",
              "      <td>0.127510</td>\n",
              "      <td>0.088359</td>\n",
              "      <td>0.163510</td>\n",
              "      <td>-0.216340</td>\n",
              "      <td>-0.094375</td>\n",
              "      <td>0.018324</td>\n",
              "      <td>0.210480</td>\n",
              "      <td>-0.030880</td>\n",
              "      <td>-0.197220</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>-0.094340</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.064699</td>\n",
              "      <td>-0.260440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.268180</td>\n",
              "      <td>0.143460</td>\n",
              "      <td>-0.27877</td>\n",
              "      <td>0.016257</td>\n",
              "      <td>0.113840</td>\n",
              "      <td>0.699230</td>\n",
              "      <td>-0.51332</td>\n",
              "      <td>-0.473680</td>\n",
              "      <td>-0.330750</td>\n",
              "      <td>-0.138340</td>\n",
              "      <td>0.270200</td>\n",
              "      <td>0.309380</td>\n",
              "      <td>-0.45012</td>\n",
              "      <td>-0.412700</td>\n",
              "      <td>-0.09932</td>\n",
              "      <td>0.038085</td>\n",
              "      <td>0.029749</td>\n",
              "      <td>0.100760</td>\n",
              "      <td>-0.25058</td>\n",
              "      <td>-0.518180</td>\n",
              "      <td>0.345580</td>\n",
              "      <td>0.449220</td>\n",
              "      <td>0.487910</td>\n",
              "      <td>-0.080866</td>\n",
              "      <td>-0.101210</td>\n",
              "      <td>-1.3777</td>\n",
              "      <td>-0.108660</td>\n",
              "      <td>-0.232010</td>\n",
              "      <td>0.012839</td>\n",
              "      <td>-0.465080</td>\n",
              "      <td>3.8463</td>\n",
              "      <td>0.313620</td>\n",
              "      <td>0.136430</td>\n",
              "      <td>-0.522440</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>0.337070</td>\n",
              "      <td>-0.356010</td>\n",
              "      <td>0.324310</td>\n",
              "      <td>0.120410</td>\n",
              "      <td>0.351200</td>\n",
              "      <td>-0.069043</td>\n",
              "      <td>0.368850</td>\n",
              "      <td>0.251680</td>\n",
              "      <td>-0.245170</td>\n",
              "      <td>0.253810</td>\n",
              "      <td>0.136700</td>\n",
              "      <td>-0.311780</td>\n",
              "      <td>-0.632100</td>\n",
              "      <td>-0.250280</td>\n",
              "      <td>-0.380970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>0.330420</td>\n",
              "      <td>0.249950</td>\n",
              "      <td>-0.60874</td>\n",
              "      <td>0.109230</td>\n",
              "      <td>0.036372</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>-0.55083</td>\n",
              "      <td>-0.074239</td>\n",
              "      <td>-0.092307</td>\n",
              "      <td>-0.328210</td>\n",
              "      <td>0.095980</td>\n",
              "      <td>-0.822690</td>\n",
              "      <td>-0.36717</td>\n",
              "      <td>-0.670090</td>\n",
              "      <td>0.42909</td>\n",
              "      <td>0.016496</td>\n",
              "      <td>-0.235730</td>\n",
              "      <td>0.128640</td>\n",
              "      <td>-1.09530</td>\n",
              "      <td>0.433340</td>\n",
              "      <td>0.570670</td>\n",
              "      <td>-0.103600</td>\n",
              "      <td>0.204220</td>\n",
              "      <td>0.078308</td>\n",
              "      <td>-0.427950</td>\n",
              "      <td>-1.7984</td>\n",
              "      <td>-0.278650</td>\n",
              "      <td>0.119540</td>\n",
              "      <td>-0.126890</td>\n",
              "      <td>0.031744</td>\n",
              "      <td>3.8631</td>\n",
              "      <td>-0.177860</td>\n",
              "      <td>-0.082434</td>\n",
              "      <td>-0.626980</td>\n",
              "      <td>0.264970</td>\n",
              "      <td>-0.057185</td>\n",
              "      <td>-0.073521</td>\n",
              "      <td>0.461030</td>\n",
              "      <td>0.308620</td>\n",
              "      <td>0.124980</td>\n",
              "      <td>-0.486090</td>\n",
              "      <td>-0.008027</td>\n",
              "      <td>0.031184</td>\n",
              "      <td>-0.365760</td>\n",
              "      <td>-0.426990</td>\n",
              "      <td>0.421640</td>\n",
              "      <td>-0.116660</td>\n",
              "      <td>-0.507030</td>\n",
              "      <td>-0.027273</td>\n",
              "      <td>-0.532850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.217050</td>\n",
              "      <td>0.465150</td>\n",
              "      <td>-0.46757</td>\n",
              "      <td>0.100820</td>\n",
              "      <td>1.013500</td>\n",
              "      <td>0.748450</td>\n",
              "      <td>-0.53104</td>\n",
              "      <td>-0.262560</td>\n",
              "      <td>0.168120</td>\n",
              "      <td>0.131820</td>\n",
              "      <td>-0.249090</td>\n",
              "      <td>-0.441850</td>\n",
              "      <td>-0.21739</td>\n",
              "      <td>0.510040</td>\n",
              "      <td>0.13448</td>\n",
              "      <td>-0.431410</td>\n",
              "      <td>-0.031230</td>\n",
              "      <td>0.206740</td>\n",
              "      <td>-0.78138</td>\n",
              "      <td>-0.201480</td>\n",
              "      <td>-0.097401</td>\n",
              "      <td>0.160880</td>\n",
              "      <td>-0.618360</td>\n",
              "      <td>-0.185040</td>\n",
              "      <td>-0.124610</td>\n",
              "      <td>-2.2526</td>\n",
              "      <td>-0.223210</td>\n",
              "      <td>0.504300</td>\n",
              "      <td>0.322570</td>\n",
              "      <td>0.153130</td>\n",
              "      <td>3.9636</td>\n",
              "      <td>-0.713650</td>\n",
              "      <td>-0.670120</td>\n",
              "      <td>0.283880</td>\n",
              "      <td>0.217380</td>\n",
              "      <td>0.144330</td>\n",
              "      <td>0.259260</td>\n",
              "      <td>0.234340</td>\n",
              "      <td>0.427400</td>\n",
              "      <td>-0.444510</td>\n",
              "      <td>0.138130</td>\n",
              "      <td>0.369730</td>\n",
              "      <td>-0.642890</td>\n",
              "      <td>0.024142</td>\n",
              "      <td>-0.039315</td>\n",
              "      <td>-0.260370</td>\n",
              "      <td>0.120170</td>\n",
              "      <td>-0.043782</td>\n",
              "      <td>0.410130</td>\n",
              "      <td>0.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"</th>\n",
              "      <td>0.257690</td>\n",
              "      <td>0.456290</td>\n",
              "      <td>-0.76974</td>\n",
              "      <td>-0.376790</td>\n",
              "      <td>0.592720</td>\n",
              "      <td>-0.063527</td>\n",
              "      <td>0.20545</td>\n",
              "      <td>-0.573850</td>\n",
              "      <td>-0.290090</td>\n",
              "      <td>-0.136620</td>\n",
              "      <td>0.327280</td>\n",
              "      <td>1.471900</td>\n",
              "      <td>-0.73681</td>\n",
              "      <td>-0.120360</td>\n",
              "      <td>0.71354</td>\n",
              "      <td>-0.460980</td>\n",
              "      <td>0.652480</td>\n",
              "      <td>0.488870</td>\n",
              "      <td>-0.51558</td>\n",
              "      <td>0.039951</td>\n",
              "      <td>-0.343070</td>\n",
              "      <td>-0.014087</td>\n",
              "      <td>0.864880</td>\n",
              "      <td>0.354600</td>\n",
              "      <td>0.799900</td>\n",
              "      <td>-1.4995</td>\n",
              "      <td>-1.815300</td>\n",
              "      <td>0.411280</td>\n",
              "      <td>0.239210</td>\n",
              "      <td>-0.431390</td>\n",
              "      <td>3.6623</td>\n",
              "      <td>-0.798340</td>\n",
              "      <td>-0.545380</td>\n",
              "      <td>0.169430</td>\n",
              "      <td>-0.820170</td>\n",
              "      <td>-0.346100</td>\n",
              "      <td>0.694950</td>\n",
              "      <td>-1.225600</td>\n",
              "      <td>-0.179920</td>\n",
              "      <td>-0.057474</td>\n",
              "      <td>0.030498</td>\n",
              "      <td>-0.395430</td>\n",
              "      <td>-0.385150</td>\n",
              "      <td>-1.000200</td>\n",
              "      <td>0.087599</td>\n",
              "      <td>-0.310090</td>\n",
              "      <td>-0.346770</td>\n",
              "      <td>-0.314380</td>\n",
              "      <td>0.750040</td>\n",
              "      <td>0.970650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>'s</th>\n",
              "      <td>0.237270</td>\n",
              "      <td>0.404780</td>\n",
              "      <td>-0.20547</td>\n",
              "      <td>0.588050</td>\n",
              "      <td>0.655330</td>\n",
              "      <td>0.328670</td>\n",
              "      <td>-0.81964</td>\n",
              "      <td>-0.232360</td>\n",
              "      <td>0.274280</td>\n",
              "      <td>0.242650</td>\n",
              "      <td>0.054992</td>\n",
              "      <td>0.162960</td>\n",
              "      <td>-1.25550</td>\n",
              "      <td>-0.086437</td>\n",
              "      <td>0.44536</td>\n",
              "      <td>0.096561</td>\n",
              "      <td>-0.165190</td>\n",
              "      <td>0.058378</td>\n",
              "      <td>-0.38598</td>\n",
              "      <td>0.086977</td>\n",
              "      <td>0.003387</td>\n",
              "      <td>0.550950</td>\n",
              "      <td>-0.776970</td>\n",
              "      <td>-0.620960</td>\n",
              "      <td>0.092948</td>\n",
              "      <td>-2.5685</td>\n",
              "      <td>-0.677390</td>\n",
              "      <td>0.101510</td>\n",
              "      <td>-0.486430</td>\n",
              "      <td>-0.057805</td>\n",
              "      <td>3.1859</td>\n",
              "      <td>-0.017554</td>\n",
              "      <td>-0.161380</td>\n",
              "      <td>0.055486</td>\n",
              "      <td>-0.258850</td>\n",
              "      <td>-0.339380</td>\n",
              "      <td>-0.199280</td>\n",
              "      <td>0.260490</td>\n",
              "      <td>0.104780</td>\n",
              "      <td>-0.559340</td>\n",
              "      <td>-0.123420</td>\n",
              "      <td>0.659610</td>\n",
              "      <td>-0.518020</td>\n",
              "      <td>-0.829950</td>\n",
              "      <td>-0.082739</td>\n",
              "      <td>0.281550</td>\n",
              "      <td>-0.423000</td>\n",
              "      <td>-0.273780</td>\n",
              "      <td>-0.007901</td>\n",
              "      <td>-0.030231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>0.152720</td>\n",
              "      <td>0.361810</td>\n",
              "      <td>-0.22168</td>\n",
              "      <td>0.066051</td>\n",
              "      <td>0.130290</td>\n",
              "      <td>0.370750</td>\n",
              "      <td>-0.75874</td>\n",
              "      <td>-0.447220</td>\n",
              "      <td>0.225630</td>\n",
              "      <td>0.102080</td>\n",
              "      <td>0.054225</td>\n",
              "      <td>0.134940</td>\n",
              "      <td>-0.43052</td>\n",
              "      <td>-0.213400</td>\n",
              "      <td>0.56139</td>\n",
              "      <td>-0.214450</td>\n",
              "      <td>0.077974</td>\n",
              "      <td>0.101370</td>\n",
              "      <td>-0.51306</td>\n",
              "      <td>-0.402950</td>\n",
              "      <td>0.406390</td>\n",
              "      <td>0.233090</td>\n",
              "      <td>0.206960</td>\n",
              "      <td>-0.126680</td>\n",
              "      <td>-0.506340</td>\n",
              "      <td>-1.7131</td>\n",
              "      <td>0.077183</td>\n",
              "      <td>-0.391380</td>\n",
              "      <td>-0.105940</td>\n",
              "      <td>-0.237430</td>\n",
              "      <td>3.9552</td>\n",
              "      <td>0.665960</td>\n",
              "      <td>-0.618410</td>\n",
              "      <td>-0.326800</td>\n",
              "      <td>0.370210</td>\n",
              "      <td>0.257640</td>\n",
              "      <td>0.389770</td>\n",
              "      <td>0.271210</td>\n",
              "      <td>0.043024</td>\n",
              "      <td>-0.343220</td>\n",
              "      <td>0.020339</td>\n",
              "      <td>0.214200</td>\n",
              "      <td>0.044097</td>\n",
              "      <td>0.140030</td>\n",
              "      <td>-0.200790</td>\n",
              "      <td>0.074794</td>\n",
              "      <td>-0.360760</td>\n",
              "      <td>0.433820</td>\n",
              "      <td>-0.084617</td>\n",
              "      <td>0.121400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77b92f34-8eef-4a64-bc0a-a93819cba6b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77b92f34-8eef-4a64-bc0a-a93819cba6b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77b92f34-8eef-4a64-bc0a-a93819cba6b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           1         2        3   ...        48        49        50\n",
              "0                                 ...                              \n",
              "the  0.418000  0.249680 -0.41242  ... -0.184110 -0.115140 -0.785810\n",
              ",    0.013441  0.236820 -0.16899  ... -0.566570  0.044691  0.303920\n",
              ".    0.151640  0.301770 -0.16763  ... -0.356520  0.016413  0.102160\n",
              "of   0.708530  0.570880 -0.47160  ... -0.225620 -0.093918 -0.803750\n",
              "to   0.680470 -0.039263  0.30186  ... -0.073297 -0.064699 -0.260440\n",
              "and  0.268180  0.143460 -0.27877  ... -0.632100 -0.250280 -0.380970\n",
              "in   0.330420  0.249950 -0.60874  ... -0.507030 -0.027273 -0.532850\n",
              "a    0.217050  0.465150 -0.46757  ... -0.043782  0.410130  0.179600\n",
              "\"    0.257690  0.456290 -0.76974  ... -0.314380  0.750040  0.970650\n",
              "'s   0.237270  0.404780 -0.20547  ... -0.273780 -0.007901 -0.030231\n",
              "for  0.152720  0.361810 -0.22168  ...  0.433820 -0.084617  0.121400\n",
              "\n",
              "[11 rows x 50 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "# Your code HERE - It should execute as expected! \n",
        "# (Search for a pandas functionality that can help you do this!)\n",
        "df.iloc[:11]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vt9UZMQDgbH"
      },
      "source": [
        "## \\# 2. Words Similarity\n",
        "\n",
        "Similar words have similar embeddings (or vector values). We can use cosine similarity i.e. cos(u,v) = u.v/(|u||v|) to measure vector similarity. u.v is dot product of vectors, |u| is L2 norm of u. Remember, we spoke about computing similarity based on cosine-similarity (as another alternative to correlation) in class?\n",
        "\n",
        "1. Normalize matrix df by norm of word vectors. \n",
        "1. Define a function to find words similarity to a given word.\n",
        "1. Use the function defined to find the word in examples that is most similar to \"happy\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulqGomrRDgbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69807503-972a-4afe-d46d-907ee5f3a245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       value         words\n",
            "9   0.865877          glad\n",
            "13  0.816272        hardly\n",
            "12  0.747581      probably\n",
            "1   0.708395           bad\n",
            "0   0.689063           sad\n",
            "3   0.640579       healthy\n",
            "18  0.599150          word\n",
            "6   0.575719      cheerful\n",
            "10  0.566424         upset\n",
            "7   0.555032        joyful\n",
            "15  0.554029         close\n",
            "4   0.522978           ill\n",
            "2   0.452148          evil\n",
            "11  0.324669         disco\n",
            "5   0.289510       beaming\n",
            "16  0.246022      cleaning\n",
            "19  0.160149  distribution\n",
            "8   0.134971       radiant\n",
            "14  0.132886     ephemeral\n",
            "17 -0.011991         maths\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "## YOUR CODE HERE\n",
        "# 1a. Calculate norm of word vectors\n",
        "# What would be the dimension of the vector_norm array?\n",
        "df_array = np.array(df)\n",
        "m,n = np.shape(df_array)\n",
        "vector_norm = []\n",
        "for i in range(m):\n",
        "  vec = np.sqrt(sum([key**2 for key in df_array[i]]))\n",
        "  vector_norm.append(vec)\n",
        "  \n",
        "vector_norm = pd.DataFrame(vector_norm)\n",
        "vector_norm.insert(0,'words',df.index.values)\n",
        "vector_norm = vector_norm.set_index(['words'])# dimension:(40001,1)\n",
        "\n",
        "# 1b. Normalize matrix df by norm using .div()\n",
        "vector_norm = vector_norm.T\n",
        "vector_norm = pd.Series(list(vector_norm.iloc[0]))\n",
        "dfn = df.div(vector_norm, axis = 1)\n",
        "# dfn= preprocessing.normalize(df, norm='l2')\n",
        "# dfn = pd.DataFrame(dfn)\n",
        "# dfn.insert(50,'words',df.index.values)\n",
        "# dfn = dfn.set_index(['words'])\n",
        "# 2. Define a function to find words similar to a given word in a normalized dataframe\n",
        "\n",
        "def find_word_similarity(word, examples,dataframe):\n",
        "    # Input: word - one string\n",
        "    #        examples - List of strings\n",
        "    #        dataframe - An indexed normalized dataframe\n",
        "    ## YOUR CODE HERE\n",
        "    # Calculate dot product of each word in examples to the given word, sorted by value high to low\n",
        "    # Once you have the sorted values of dot products (notice because of normalization, the dot product is the cosine similarity!),\n",
        "    # obtain the words corresponding to the sorted values and call it similar_words\n",
        "  similar_words = pd.DataFrame(columns=['value','words'])\n",
        "  i = 0\n",
        "  for key in examples:\n",
        "    cos_value = float(np.dot(dataframe.loc[word].values.tolist(),dataframe.loc[key].values.tolist())/(vector_norm.loc[word]*vector_norm.loc[key]))\n",
        "    similar_words.loc[i] = [cos_value,key]\n",
        "    i += 1\n",
        "  similar_words = similar_words.sort_values(by = 'value',ascending = False)\n",
        "\n",
        "    # Return words similar to the given word\n",
        "  return similar_words\n",
        "    \n",
        "examples = [\"sad\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
        "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
        "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
        "            \"maths\", \"word\", \"distribution\"]\n",
        "\n",
        "# 3.\n",
        "# Use above function to calculate examples' similarity to happy (both \"happy\" and words in examples are in dfn)\n",
        "print (find_word_similarity(\"happy\", examples,df))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The word that most similar to 'happy' is glad"
      ],
      "metadata": {
        "id": "xWr2DMcMpR7S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZoAZi-EDgbH"
      },
      "source": [
        "In sklean library,there is a cosine_similarity fuction that directly calcualtes vectors similarity (you don't need to normalize vectors first). Let's use this function to calculate similarity again to confirm we get same results. \n",
        "For more information, please see here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2hs1IiXDgbI"
      },
      "outputs": [],
      "source": [
        "examples = [\"sad\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
        "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
        "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
        "            \"maths\", \"word\", \"distribution\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4KzSBtoDgbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e690a33-252d-41d3-fa8b-5779556149c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     0\n",
            "glad          0.865877\n",
            "hardly        0.816272\n",
            "probably      0.747581\n",
            "bad           0.708395\n",
            "sad           0.689063\n",
            "healthy       0.640579\n",
            "word          0.599150\n",
            "cheerful      0.575719\n",
            "upset         0.566424\n",
            "joyful        0.555032\n",
            "close         0.554029\n",
            "ill           0.522978\n",
            "evil          0.452148\n",
            "disco         0.324669\n",
            "beaming       0.289510\n",
            "cleaning      0.246022\n",
            "distribution  0.160149\n",
            "radiant       0.134971\n",
            "ephemeral     0.132886\n",
            "maths        -0.011991\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity2 = pd.DataFrame(cosine_similarity(np.array(df.loc['happy']).reshape(1,50),np.array(df.loc[examples])),columns = examples)\n",
        "similarity2 = similarity2.T\n",
        "similarity2 = similarity2.sort_values(by = 0,ascending = False)\n",
        "print(similarity2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd3S_ZrKDgbI"
      },
      "source": [
        "## \\# 3. Goodness of similarity\n",
        "Comment on the how good the glove embeddings are on finding similar words to a given word using cosine similarity? Glove and word2vec embeddings are based on co-occurence of words in senetences across hundreds of thousands of documents on the web. Would this help explain your observations on word similarity?\n",
        "What if you replace happy with sad, how do the results change?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "As a word vector representation algorithm, Glove can make full use of the global statistics of LSA and the local context based of word2vec.Aditionally, Golve is very efficient, and its computational scale is proportional to the size of the corpus. When the corpus is not large enough or the word vector dimension is small, it can still maintain a good effect.\n",
        "\n",
        "Yes, it would. Documents on the web are ubiquitous, that is to say, they contain almost all forms of human language, including formal, informal, scholarly articles, novels, even Old English, slang and some uncommon usages. This greatly enriches the type and capacity of the corpus, which is enough for the model to fully learn to make judgments, and does not make the model enter some local solutions. This has great benefits for word similarity judgment."
      ],
      "metadata": {
        "id": "nVCk4DHITtLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (find_word_similarity(\"sad\", examples,df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoMRtdWvbn3l",
        "outputId": "51524f85-1db0-4549-c9ec-5601e408334c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       value         words\n",
            "0   1.000000           sad\n",
            "13  0.707888        hardly\n",
            "1   0.664580           bad\n",
            "9   0.644321          glad\n",
            "7   0.587249        joyful\n",
            "18  0.559652          word\n",
            "12  0.535477      probably\n",
            "6   0.531766      cheerful\n",
            "10  0.501137         upset\n",
            "4   0.487844           ill\n",
            "2   0.401493          evil\n",
            "3   0.372976       healthy\n",
            "11  0.362746         disco\n",
            "15  0.311398         close\n",
            "8   0.244188       radiant\n",
            "14  0.234799     ephemeral\n",
            "5   0.194717       beaming\n",
            "16  0.135319      cleaning\n",
            "17  0.030129         maths\n",
            "19  0.008951  distribution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PkJ-gjcDgbI"
      },
      "source": [
        "## \\# 4. Correlations\n",
        "(This question is more of a reflection and building your intuition on how correlations we spoke in class connects to a real-world data set -  Open ended!)\n",
        "What are some of the most correlated words from the similarity search you did earlier to the word \"happy\" and \"sad\". Likewise, what are some of the most uncorrelated words to \"happy\" and \"sad\". Does this make sense? How would you improve the results ? If \"happy\" were a random variable and \"sad\" was a random variable - What factors make the correlation between \"happy\" and \"sad\" (as you computed above) high?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The word that most correlated to both '**happy**' and '**sad**' is '**hardly**'. It does not make sense. \n",
        "\n",
        "And the word that most uncorrelated is '**maths**', I suppose that it makes sense. \n",
        "\n",
        "I think I should improve my model that adding some restrictions and conditions which could keep some uncorrelated words like 'hardly' away from the input word.\n",
        "A practical method that I prefer is to introduce a detector--a word that different from inputs and results--to show the relationships  between inputs and outputs. The relationships could be cosine similarity, dot product of vectors or something else.\n",
        "\n",
        "In my opinion, both **sad** and **happy** are emotional words that people always say, maybe they could appear simultaneously in some sentences which people express their thoughts, feelings, or do some comparisons.Addtiionally, they would appear in some cases where some other eomtional words in, for example, glad, upset, frustrated, angry and so on, which will also lead into a high correlation. Thus, the correlation between happy and sad is relatively high."
      ],
      "metadata": {
        "id": "YcUHOQPvfoPG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8CUGbmBDgbI"
      },
      "source": [
        "## \\# 5. Find nearest neighbourhood\n",
        "\n",
        "It is helpful to compute the nearest neighbors to a word based on the cosine similarity that we defined earlier, so that given a word we can compute which are the other words which are most similar to it. Sometimes, the nearest neighbors according to this metric reveal overlap of concepts or topics that a word shares. E.g. government might be related to the word politics because they both share topics related to public policy, politicians, parties, elections, etc. The idea is whatever embeddings we are using - word2vec or glove is \"hopefully\" able to capture these correlations right!\n",
        "\n",
        "1. Define a function to find the top n similar words to a given word. You can use either dot product of vectors or cosine_simialrity function. Note the search space for words is coming from your pandas data frame (so unlike the similarity problem we worked on earlier, we are not restricted to only a few words to search from - the search space here is the entire vocab captured in your data frame).\n",
        "1. Find 20 nearest neighborhood for words 'duck' and 'animal'.\n",
        "1. Find neighborhood intersection of 'duck' and 'animal', to find which words are similar to both 'duck' and 'animal'. This is also related to a similarity measure called \"Jaccard Similarity\" - Read up on this here: https://en.wikipedia.org/wiki/Jaccard_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-o1NzC7DgbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342ef692-63fd-4504-81e3-ded794ea9460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             0         0\n",
            "pig   0.739596  0.735997\n",
            "fish  0.676019  0.728633\n",
            "dog   0.693291  0.725226\n"
          ]
        }
      ],
      "source": [
        "# define a function to find the top n similar words to a given word in the 'df'\n",
        "\n",
        "# PART 1\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def find_most_similar(df, word, n):\n",
        "    # INPUT: \n",
        "    # df: Given Data frame\n",
        "    # word: String\n",
        "    # n: Number of similar words to return\n",
        "    \n",
        "    # OUTPUT:\n",
        "    # the list of similar words to return\n",
        "    \n",
        "    ## YOUR CODE HERE\n",
        "    # define and compute the most similar words\n",
        "    # Use a similarity measure like cosine similarity (like earlier) to do so\n",
        "  similarity2 = pd.DataFrame(cosine_similarity(np.array(df.loc[word]).reshape(1,50),np.array(df)),columns = df.index.values.tolist())\n",
        "  similarity2 = similarity2.T\n",
        "  similarity2 = similarity2.sort_values(by = 0,ascending = False)\n",
        "  similar_words = similarity2.iloc[:n]\n",
        "  return similar_words\n",
        "\n",
        "\n",
        "# PART 2\n",
        "# find top 20 similar words to duck\n",
        "simil1 = find_most_similar(df, \"duck\", 20)\n",
        "# find top 20 similar words to animal\n",
        "simil2 = find_most_similar(df, \"animal\", 20)\n",
        "\n",
        "# PART 3\n",
        "# find the intersection of simil1 and simil2\n",
        "#intersection =  (concat function of pandas is helpful here)\n",
        "intersection = [key for key in simil2.index if key in simil1.index]\n",
        "intersection = pd.concat([simil1.loc[intersection],simil2.loc[intersection]],axis = 1)\n",
        "print (intersection)\n",
        "\n",
        "word_labels = ['duck', 'animal'] + list(intersection.index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(simil1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnjCB8m-jY3_",
        "outputId": "308929c8-ed59-4abd-f664-2480f64e8a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 0\n",
            "duck      1.000000\n",
            "crab      0.775702\n",
            "lobster   0.762002\n",
            "lame      0.746981\n",
            "rabbit    0.745671\n",
            "pig       0.739596\n",
            "goose     0.735946\n",
            "chicken   0.725202\n",
            "grilled   0.722439\n",
            "fried     0.712361\n",
            "shrimp    0.707118\n",
            "cat       0.703251\n",
            "dog       0.693291\n",
            "darkwing  0.685793\n",
            "goat      0.681566\n",
            "monkey    0.678014\n",
            "confit    0.677600\n",
            "fish      0.676019\n",
            "bite      0.673196\n",
            "broiled   0.670877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(simil2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3JFSFD_pun2",
        "outputId": "84f2afa5-e4d3-4f4b-e405-c3b7cdefb786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  0\n",
            "animal     1.000000\n",
            "animals    0.900348\n",
            "bird       0.800324\n",
            "human      0.772533\n",
            "dogs       0.758666\n",
            "pet        0.747176\n",
            "pig        0.735997\n",
            "feeding    0.731338\n",
            "fish       0.728633\n",
            "insect     0.727963\n",
            "humans     0.726976\n",
            "pigs       0.726021\n",
            "dog        0.725226\n",
            "elephant   0.723749\n",
            "found      0.722678\n",
            "cow        0.722619\n",
            "birds      0.719840\n",
            "livestock  0.719640\n",
            "eating     0.714771\n",
            "breeding   0.705041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# jaccard similarity\n",
        "def jaccard(p,q):\n",
        "    c = [a for i in p if i in q]\n",
        "    return float(len(c))/(len(a)+len(b)-len(c))\n",
        "\n",
        "print('the jaccard similarity is :' ,'%.4f' % jaccard(simil1,simil2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7nrCg89fI_9",
        "outputId": "156af4ba-a037-4654-e671-604674253f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the jaccard similarity is : 0.1111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1kaFPa6DgbJ"
      },
      "source": [
        "## \\# 6 Word analogies\n",
        "\n",
        "Suppose you know the word vectors for King, Man and Woman. What is your intuitive answer for the 'riddle', King - Man + Woman = ? \n",
        "Let's go through below steps to derive the answer for this 'riddle' using the word embeddings.\n",
        "\n",
        "1. Use vector arithmetic to define a new vector which equals to k - m + w (e.g. king, man and woman combination).\n",
        "2. Calculate similarity of all the words in the corpus to the new vector and sort them by their similarity high to low. \n",
        "3. Return the top n vectors which have the highest similarity to the new vector.\n",
        "1. Find the answers for the riddles, \n",
        "    1. good:bad::up:?\n",
        "    1. germany:merkel::america:?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_withoutkeywords(df, vec, n,x,y,a):#vec: new vector which is not in df\n",
        "  similarity2 = pd.DataFrame(cosine_similarity(vec.reshape(1,50),np.array(df)),columns = df.index.values.tolist())\n",
        "  similarity2 = similarity2.T\n",
        "  similarity2 = similarity2.sort_values(by = 0,ascending = False)\n",
        "  similarity2.drop([x,y,a],inplace = True)\n",
        "  similar_words = similarity2.iloc[:n]\n",
        "  return similar_words"
      ],
      "metadata": {
        "id": "x3YEZenZ7r7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_similar_withkeywords(df, vec, n):#vec: new vector which is not in df\n",
        "  similarity2 = pd.DataFrame(cosine_similarity(vec.reshape(1,50),np.array(df)),columns = df.index.values.tolist())\n",
        "  similarity2 = similarity2.T\n",
        "  similarity2 = similarity2.sort_values(by = 0,ascending = False)\n",
        "  similar_words = similarity2.iloc[:n]\n",
        "  return similar_words"
      ],
      "metadata": {
        "id": "v8-nYjdR8aqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o0oQZJyDgbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f524ab-48fe-4cad-b73c-5d7e713cc7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the result of riddle with known words is:\n",
            "                 0\n",
            "king      0.885983\n",
            "queen     0.860958\n",
            "daughter  0.768451\n",
            "prince    0.764070\n",
            "throne    0.763497\n",
            "the result of riddle without known words is:\n",
            "                 0\n",
            "queen     0.860958\n",
            "daughter  0.768451\n",
            "prince    0.764070\n",
            "throne    0.763497\n",
            "princess  0.751273\n"
          ]
        }
      ],
      "source": [
        "# define a function to solve the problem of x is to y as a is to ?\n",
        "# 'n' is the number of top words similar to the vector to return\n",
        "# 'dataframe' is the indexed dataframe that contains all the words\n",
        "\n",
        "# PART 1,2,3 above (Fill in the missing pieces)\n",
        "def solve_riddle(x, y, a, n, dataframe):\n",
        "    ## YOUR CODE HERE\n",
        "    # calculate the vector of a + y - x, where a, x, y are in dataframe\n",
        "    #x man\n",
        "    #y woman\n",
        "    #a king\n",
        "    cal_vec = np.array(dataframe.loc[a] + dataframe.loc[y] - dataframe.loc[x])\n",
        "    \n",
        "    # calculate distance of words in dataframe to cal_vec, sorted by similarity high to low\n",
        "    similarity_with = find_most_similar_withkeywords(df, cal_vec, n)\n",
        "    similarity_without = find_most_similar_withoutkeywords(df, cal_vec, n,x,y,a)\n",
        "\n",
        "    # return top n words and distance that closest to cal_vec\n",
        "    return similarity_with, similarity_without\n",
        "result_with_keywords, result_without_keywords = solve_riddle(\"man\", \"woman\", \"king\", 5,df)\n",
        "# Call the solve_riddle function to compute the top answers\n",
        "print('the result of riddle with known words is:')\n",
        "print(result_with_keywords)\n",
        "print('the result of riddle without known words is:')\n",
        "print(result_without_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is '**queen**'"
      ],
      "metadata": {
        "id": "G1bEHAz-9vaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## YOUR CODE HERE\n",
        "# Solve the other two riddles\n",
        "# good:bad::up:?\n",
        "# PART 4\n",
        "#print solve_riddle()\n",
        "result_with_keywords2, result_without_keywords2 = solve_riddle(\"good\", \"bad\", \"up\", 5,df)\n",
        "# Call the solve_riddle function to compute the top answers\n",
        "print('the result of riddle with known words is:')\n",
        "print(result_with_keywords2)\n",
        "print('the result of riddle without known words is:')\n",
        "print(result_without_keywords2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyBdtiXS95o-",
        "outputId": "05dae853-cb1a-4a06-af82-7461a50d9ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the result of riddle with known words is:\n",
            "                 0\n",
            "down      0.850167\n",
            "up        0.818045\n",
            "falling   0.813844\n",
            "out       0.792837\n",
            "dropping  0.782064\n",
            "the result of riddle without known words is:\n",
            "                 0\n",
            "down      0.850167\n",
            "falling   0.813844\n",
            "out       0.792837\n",
            "dropping  0.782064\n",
            "off       0.778428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is '**down**'"
      ],
      "metadata": {
        "id": "EZFFBDe8-izN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# germany:merkel::america:?\n",
        "#print solve_riddle()\n",
        "result_with_keywords3, result_without_keywords3 = solve_riddle(\"germany\", \"merkel\", \"america\", 5,df)\n",
        "# Call the solve_riddle function to compute the top answers\n",
        "print('the result of riddle with known words is:')\n",
        "print(result_with_keywords3)\n",
        "print('the result of riddle without known words is:')\n",
        "print(result_without_keywords3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0YvWR96-a0S",
        "outputId": "351f7f40-ce3b-4c8c-bd14-8cd9aa996ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the result of riddle with known words is:\n",
            "                0\n",
            "obama    0.694289\n",
            "barack   0.682594\n",
            "hillary  0.660910\n",
            "bush     0.657911\n",
            "clinton  0.655765\n",
            "the result of riddle without known words is:\n",
            "                0\n",
            "obama    0.694289\n",
            "barack   0.682594\n",
            "hillary  0.660910\n",
            "bush     0.657911\n",
            "clinton  0.655765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer is '**obama**' or '**barack obama**'"
      ],
      "metadata": {
        "id": "J7bMzKK4-knc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "596A PG4 Xiangyu Gao.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}